{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungarian-nirvana",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, time, csv, webbrowser\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime,date\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-closure",
   "metadata": {},
   "source": [
    "### Parsing into txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-convention",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sn=1\n",
    "#We use 'c' as both the serial number and the time count for bypassing anti-crawler\n",
    "# datentime=str(date.today())\n",
    "# datentime+=\" \"\n",
    "datentime=datetime.now().strftime(\"%H:%M:%S\")\n",
    "print(\"Started at \"+datentime)\n",
    "with open('lawyer_ci/Appellate Devision/files.txt','a',encoding=\"utf-8\")as outfile, open(\"lawyer_ci\\Appellate Devision/links.txt\",\"r\",encoding='utf-8') as infile:\n",
    "#     outfile.write(\"This is a test\\n\")\n",
    "    for line in infile:\n",
    "        if(sn<1):\n",
    "            sn+=1\n",
    "            continue\n",
    "        url=\"\"\n",
    "        for i in line:\n",
    "            if(i!='\\n'):\n",
    "                url+=i\n",
    "        print(\"%d \\t\"%sn,end='')\n",
    "        page = requests.get(url,headers=headers)\n",
    "        soup = BeautifulSoup(page.content, 'lxml')\n",
    "        title=soup.find(class_=\"blog_entry-title\")\n",
    "        y=str(soup.find(class_=\"entry-content\"))\n",
    "        it=1\n",
    "        while('None' in y):\n",
    "            if(it>5):break\n",
    "            page = requests.get(url,headers=headers)\n",
    "            soup = BeautifulSoup(page.content, 'lxml')\n",
    "            title=soup.find(class_=\"blog_entry-title\")\n",
    "            y=str(soup.find(class_=\"entry-content\"))\n",
    "            it+=1\n",
    "        content=y.replace('\\n','').replace('\\r\\n','').replace('\\r','')\n",
    "        outfile.write(\"%d\\t%s\\n%s\\n\"%(sn,title,content))\n",
    "        sn+=1\n",
    "        if(sn%10==1):\n",
    "#             datentime=str(date.today())\n",
    "#             datentime+=\" \"\n",
    "            datentime=datetime.now().strftime(\"%H:%M:%S\")\n",
    "            print(\"\\n\"+datentime+\"\\n----------------------------------------------------------------\")\n",
    "        # if(sn>=1500):break\n",
    "#         if(sn>1500):break\n",
    "# datentime=str(date.today())\n",
    "# datentime+=\" \"\n",
    "datentime=datetime.now().strftime(\"%H:%M:%S\")\n",
    "print(\"\\nDone at \"+datentime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-refrigerator",
   "metadata": {},
   "source": [
    "### Parsing into csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-weekly",
   "metadata": {},
   "outputs": [],
   "source": [
    "sn=1\n",
    "#We use 'c' as both the serial number and the time count for bypassing anti-crawler\n",
    "datentime=str(date.today())\n",
    "datentime+=\" \"\n",
    "datentime+=datetime.now().strftime(\"%H:%M:%S\")\n",
    "print(datentime)\n",
    "with open('csv_files/articles.csv','w',encoding=\"utf-8\")as outfile, open(\"text_files/article_links.txt\",\"r\",encoding='utf-8') as infile:\n",
    "    writer=csv.writer(outfile)\n",
    "    writer.writerow([\"SN\",\"Title\", \"Content\"])\n",
    "    for line in infile:\n",
    "        print(\"%d \\t\"%sn,end='')\n",
    "        if(sn%10==0):time.sleep(61)\n",
    "        page = requests.get(line,headers=headers)\n",
    "        soup = BeautifulSoup(page.content, 'lxml')\n",
    "        title=soup.find(class_=\"blog_entry-title\")\n",
    "        y=str(soup.find(class_=\"entry-content\"))\n",
    "        content=y.replace('\\n','').replace('\\r\\n','').replace('\\r','')\n",
    "        writer.writerow([sn,title,content])\n",
    "        sn+=1\n",
    "        if(sn==10):break\n",
    "            \n",
    "datentime=str(date.today())\n",
    "datentime+=\" \"\n",
    "datentime+=datetime.now().strftime(\"%H:%M:%S\")\n",
    "print(\"\\n\"+datentime)\n",
    "\n",
    "# There are newline characters in the articles that were fetched. This causes unorganized csv structure.\n",
    "# We have to find a new for writerow to work without considering newline characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasonable-special",
   "metadata": {},
   "source": [
    "## Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-method",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "page = requests.get(\"https://www.lawyersnjurists.com/doclit/deed-agreement-dissolution-partnership/\",headers=headers)\n",
    "soup = BeautifulSoup(page.content, 'lxml')\n",
    "# print(BeautifulSoup(soup.body,\"html.parser\").prettify())\n",
    "x=soup.find(class_=\"blog_entry-title\")\n",
    "y=soup.find(class_=\"entry-content\")\n",
    "print(x)\n",
    "print(y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python391jvsc74a57bd077eae0d149ba61c2d600661d89f90b4aa2630fec4556ce7a67e67a981106c5c1",
   "display_name": "Python 3.9.1 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "metadata": {
   "interpreter": {
    "hash": "77eae0d149ba61c2d600661d89f90b4aa2630fec4556ce7a67e67a981106c5c1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}