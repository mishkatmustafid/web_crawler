{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports and essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime,date\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn=1\n",
    "#We use 'c' as both the serial number and the time count for bypassing anti-crawler\n",
    "datentime=str(date.today())\n",
    "datentime+=\" \"\n",
    "datentime+=datetime.now().strftime(\"%H:%M:%S\")\n",
    "print(datentime)\n",
    "with open('csv_files/articles.csv','w',encoding=\"utf-8\")as outfile, open(\"text_files/article_links.txt\",\"r\",encoding='utf-8') as infile:\n",
    "    writer=csv.writer(outfile)\n",
    "    writer.writerow([\"SN\",\"Title\", \"Content\"])\n",
    "    for line in infile:\n",
    "        print(\"%d \\t\"%sn,end='')\n",
    "        if(sn%10==0):time.sleep(61)\n",
    "        page = requests.get(line,headers=headers)\n",
    "        soup = BeautifulSoup(page.content, 'lxml')\n",
    "        title=soup.find(class_=\"blog_entry-title\")\n",
    "        y=str(soup.find(class_=\"entry-content\"))\n",
    "        content=y.replace('\\n','').replace('\\r\\n','').replace('\\r','')\n",
    "        writer.writerow([sn,title,content])\n",
    "        sn+=1\n",
    "        if(sn==10):break\n",
    "            \n",
    "datentime=str(date.today())\n",
    "datentime+=\" \"\n",
    "datentime+=datetime.now().strftime(\"%H:%M:%S\")\n",
    "print(\"\\n\"+datentime)\n",
    "\n",
    "# There are newline characters in the articles that were fetched. This causes unorganized csv structure.\n",
    "# We have to find a new for writerow to work without considering newline characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempting to parse into txt file (Successful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at 2021-03-27 15:07:02\n",
      "511 \t512 \t513 \t514 \t515 \t516 \t517 \t518 \t519 \t520 \t\n",
      "2021-03-27 15:09:06\n",
      "\n",
      "521 \t522 \t523 \t524 \t525 \t526 \t527 \t528 \t529 \t530 \t\n",
      "2021-03-27 15:11:49\n",
      "\n",
      "531 \t532 \t533 \t534 \t535 \t536 \t537 \t538 \t539 \t540 \t\n",
      "2021-03-27 15:13:53\n",
      "\n",
      "541 \t542 \t543 \t544 \t545 \t546 \t547 \t548 \t549 \t550 \t\n",
      "2021-03-27 15:16:15\n",
      "\n",
      "551 \t552 \t553 \t554 \t555 \t556 \t557 \t558 \t559 \t560 \t\n",
      "2021-03-27 15:18:22\n",
      "\n",
      "561 \t562 \t563 \t564 \t565 \t566 \t567 \t568 \t569 \t570 \t\n",
      "2021-03-27 15:20:45\n",
      "\n",
      "571 \t572 \t573 \t574 \t575 \t576 \t577 \t578 \t579 \t580 \t\n",
      "2021-03-27 15:22:54\n",
      "\n",
      "581 \t582 \t583 \t584 \t585 \t586 \t587 \t588 \t589 \t590 \t\n",
      "2021-03-27 15:25:01\n",
      "\n",
      "591 \t592 \t593 \t594 \t595 \t596 \t597 \t598 \t599 \t600 \t\n",
      "2021-03-27 15:27:06\n",
      "\n",
      "601 \t602 \t603 \t604 \t605 \t606 \t607 \t608 \t609 \t610 \t\n",
      "2021-03-27 15:29:23\n",
      "\n",
      "611 \t612 \t613 \t614 \t615 \t616 \t617 \t618 \t619 \t620 \t\n",
      "2021-03-27 15:31:52\n",
      "\n",
      "621 \t622 \t623 \t624 \t625 \t626 \t627 \t628 \t629 \t630 \t\n",
      "2021-03-27 15:33:59\n",
      "\n",
      "631 \t632 \t633 \t634 \t635 \t636 \t637 \t638 \t639 \t640 \t\n",
      "2021-03-27 15:36:34\n",
      "\n",
      "641 \t642 \t643 \t644 \t645 \t646 \t647 \t648 \t649 \t650 \t\n",
      "2021-03-27 15:38:46\n",
      "\n",
      "651 \t652 \t653 \t654 \t655 \t656 \t657 \t658 \t659 \t660 \t\n",
      "2021-03-27 15:40:57\n",
      "\n",
      "661 \t662 \t663 \t664 \t665 \t666 \t667 \t668 \t669 \t670 \t\n",
      "2021-03-27 15:42:59\n",
      "\n",
      "671 \t672 \t673 \t674 \t675 \t676 \t677 \t678 \t679 \t680 \t\n",
      "2021-03-27 15:45:01\n",
      "\n",
      "681 \t682 \t683 \t684 \t685 \t686 \t687 \t688 \t689 \t690 \t\n",
      "2021-03-27 15:47:03\n",
      "\n",
      "691 \t692 \t693 \t694 \t695 \t696 \t697 \t698 \t699 \t700 \t\n",
      "2021-03-27 15:49:05\n",
      "\n",
      "701 \t702 \t703 \t704 \t705 \t706 \t707 \t708 \t709 \t710 \t\n",
      "2021-03-27 15:51:04\n",
      "\n",
      "711 \t"
     ]
    }
   ],
   "source": [
    "sn=1\n",
    "#We use 'c' as both the serial number and the time count for bypassing anti-crawler\n",
    "datentime=str(date.today())\n",
    "datentime+=\" \"\n",
    "datentime+=datetime.now().strftime(\"%H:%M:%S\")\n",
    "print(\"Started at \"+datentime)\n",
    "with open('text_files/articles.txt','a',encoding=\"utf-8\")as outfile, open(\"text_files/article_links.txt\",\"r\",encoding='utf-8') as infile:\n",
    "#     outfile.write(\"This is a test\\n\")\n",
    "    for line in infile:\n",
    "        if(sn<511):\n",
    "            sn+=1\n",
    "            continue\n",
    "        print(\"%d \\t\"%sn,end='')\n",
    "#         if(sn%10==0):time.sleep(61)\n",
    "        page = requests.get(line,headers=headers)\n",
    "        time.sleep(6)\n",
    "        soup = BeautifulSoup(page.content, 'lxml')\n",
    "        title=soup.find(class_=\"blog_entry-title\")\n",
    "        y=str(soup.find(class_=\"entry-content\"))\n",
    "        content=y.replace('\\n','').replace('\\r\\n','').replace('\\r','')\n",
    "        outfile.write(\"%d\\t%s\\n%s\\n\"%(sn,title,content))\n",
    "        sn+=1\n",
    "#         if(sn==10):break\n",
    "        if(sn%10==1):\n",
    "            datentime=str(date.today())\n",
    "            datentime+=\" \"\n",
    "            datentime+=datetime.now().strftime(\"%H:%M:%S\")\n",
    "            print(\"\\n\"+datentime+\"\\n\")\n",
    "datentime=str(date.today())\n",
    "datentime+=\" \"\n",
    "datentime+=datetime.now().strftime(\"%H:%M:%S\")\n",
    "print(\"\\nDone at \"+datentime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(\"https://www.lawyersnjurists.com/doclit/deed-agreement-dissolution-partnership/\",headers=headers)\n",
    "soup = BeautifulSoup(page.content, 'lxml')\n",
    "# print(BeautifulSoup(soup.body,\"html.parser\").prettify())\n",
    "x=soup.find(class_=\"blog_entry-title\")\n",
    "y=soup.find(class_=\"entry-content\")\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('csv_files/articles.csv', 'w') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"SN\",\"Title\", \"Content\"])\n",
    "    writer.writerow([1, x,y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=1\n",
    "s=\"Test\"\n",
    "with open('csv_files/list.csv','w')as out, open(\"text_files/test.txt\",\"r\") as f:\n",
    "    writer=csv.writer(out)\n",
    "    writer.writerow([\"SN\",\"Title\", \"Content\"])\n",
    "    for line in f:\n",
    "        writer.writerow([c,s,line.splitlines()])\n",
    "        c+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
