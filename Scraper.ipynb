{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports and essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime,date\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn=1\n",
    "#We use 'c' as both the serial number and the time count for bypassing anti-crawler\n",
    "datentime=str(date.today())\n",
    "datentime+=\" \"\n",
    "datentime+=datetime.now().strftime(\"%H:%M:%S\")\n",
    "print(datentime)\n",
    "with open('csv_files/articles.csv','w',encoding=\"utf-8\")as outfile, open(\"text_files/article_links.txt\",\"r\",encoding='utf-8') as infile:\n",
    "    writer=csv.writer(outfile)\n",
    "    writer.writerow([\"SN\",\"Title\", \"Content\"])\n",
    "    for line in infile:\n",
    "        print(\"%d \\t\"%sn,end='')\n",
    "        if(sn%10==0):time.sleep(61)\n",
    "        page = requests.get(line,headers=headers)\n",
    "        soup = BeautifulSoup(page.content, 'lxml')\n",
    "        title=soup.find(class_=\"blog_entry-title\")\n",
    "        y=str(soup.find(class_=\"entry-content\"))\n",
    "        content=y.replace('\\n','').replace('\\r\\n','').replace('\\r','')\n",
    "        writer.writerow([sn,title,content])\n",
    "        sn+=1\n",
    "        if(sn==10):break\n",
    "            \n",
    "datentime=str(date.today())\n",
    "datentime+=\" \"\n",
    "datentime+=datetime.now().strftime(\"%H:%M:%S\")\n",
    "print(\"\\n\"+datentime)\n",
    "\n",
    "# There are newline characters in the articles that were fetched. This causes unorganized csv structure.\n",
    "# We have to find a new for writerow to work without considering newline characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempting to parse into txt file (Successful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at 2021-03-27 13:16:41\n",
      "1 \t2 \t3 \t4 \t5 \t6 \t7 \t8 \t9 \t10 \t\n",
      "2021-03-27 13:18:41\n",
      "\n",
      "11 \t12 \t13 \t14 \t15 \t16 \t17 \t18 \t19 \t20 \t\n",
      "2021-03-27 13:20:43\n",
      "\n",
      "21 \t22 \t23 \t24 \t25 \t26 \t27 \t28 \t29 \t30 \t\n",
      "2021-03-27 13:22:38\n",
      "\n",
      "31 \t32 \t33 \t34 \t35 \t36 \t37 \t38 \t39 \t40 \t\n",
      "2021-03-27 13:24:43\n",
      "\n",
      "41 \t42 \t43 \t44 \t45 \t46 \t47 \t48 \t49 \t50 \t\n",
      "2021-03-27 13:26:41\n",
      "\n",
      "51 \t52 \t53 \t54 \t55 \t56 \t57 \t58 \t59 \t60 \t\n",
      "2021-03-27 13:28:43\n",
      "\n",
      "61 \t62 \t63 \t64 \t65 \t66 \t67 \t68 \t69 \t70 \t\n",
      "2021-03-27 13:30:41\n",
      "\n",
      "71 \t72 \t73 \t74 \t75 \t76 \t77 \t78 \t79 \t80 \t\n",
      "2021-03-27 13:32:44\n",
      "\n",
      "81 \t82 \t83 \t84 \t85 \t86 \t87 \t88 \t89 \t90 \t\n",
      "2021-03-27 13:34:38\n",
      "\n",
      "91 \t92 \t93 \t94 \t95 \t96 \t97 \t98 \t99 \t100 \t\n",
      "2021-03-27 13:36:43\n",
      "\n",
      "101 \t102 \t103 \t104 \t105 \t106 \t107 \t108 \t109 \t110 \t\n",
      "2021-03-27 13:38:42\n",
      "\n",
      "111 \t112 \t113 \t114 \t115 \t116 \t117 \t118 \t119 \t120 \t\n",
      "2021-03-27 13:40:39\n",
      "\n",
      "121 \t122 \t123 \t124 \t125 \t126 \t127 \t128 \t129 \t130 \t\n",
      "2021-03-27 13:42:44\n",
      "\n",
      "131 \t132 \t133 \t134 \t135 \t136 \t137 \t138 \t139 \t140 \t\n",
      "2021-03-27 13:44:42\n",
      "\n",
      "141 \t142 \t143 \t144 \t145 \t146 \t147 \t148 \t149 \t150 \t\n",
      "2021-03-27 13:46:50\n",
      "\n",
      "151 \t152 \t153 \t154 \t155 \t156 \t157 \t158 \t159 \t160 \t\n",
      "2021-03-27 13:48:42\n",
      "\n",
      "161 \t162 \t163 \t164 \t165 \t166 \t167 \t168 \t169 \t170 \t\n",
      "2021-03-27 13:50:28\n",
      "\n",
      "171 \t172 \t173 \t174 \t175 \t176 \t177 \t178 \t179 \t180 \t\n",
      "2021-03-27 13:52:14\n",
      "\n",
      "181 \t182 \t183 \t184 \t185 \t186 \t187 \t188 \t189 \t190 \t\n",
      "2021-03-27 13:53:50\n",
      "\n",
      "191 \t192 \t193 \t194 \t195 \t196 \t197 \t198 \t199 \t200 \t\n",
      "2021-03-27 13:55:52\n",
      "\n",
      "201 \t202 \t203 \t204 \t205 \t206 \t207 \t208 \t209 \t210 \t\n",
      "2021-03-27 13:57:57\n",
      "\n",
      "211 \t212 \t213 \t214 \t215 \t216 \t217 \t218 \t219 \t220 \t\n",
      "2021-03-27 13:59:46\n",
      "\n",
      "221 \t222 \t223 \t224 \t225 \t226 \t227 \t228 \t229 \t230 \t\n",
      "2021-03-27 14:01:47\n",
      "\n",
      "231 \t232 \t233 \t234 \t235 \t236 \t237 \t238 \t239 \t240 \t\n",
      "2021-03-27 14:03:44\n",
      "\n",
      "241 \t242 \t243 \t244 \t245 \t246 \t247 \t248 \t249 \t250 \t\n",
      "2021-03-27 14:05:42\n",
      "\n",
      "251 \t252 \t253 \t254 \t255 \t256 \t257 \t258 \t259 \t260 \t\n",
      "2021-03-27 14:07:49\n",
      "\n",
      "261 \t262 \t263 \t264 \t265 \t266 \t267 \t268 \t269 \t270 \t\n",
      "2021-03-27 14:09:43\n",
      "\n",
      "271 \t272 \t273 \t274 \t275 \t276 \t277 \t278 \t279 \t280 \t\n",
      "2021-03-27 14:11:46\n",
      "\n",
      "281 \t282 \t283 \t284 \t285 \t286 \t287 \t288 \t289 \t290 \t\n",
      "2021-03-27 14:13:55\n",
      "\n",
      "291 \t292 \t293 \t294 \t295 \t296 \t297 \t298 \t299 \t300 \t\n",
      "2021-03-27 14:15:50\n",
      "\n",
      "301 \t302 \t303 \t304 \t305 \t306 \t307 \t308 \t309 \t310 \t\n",
      "2021-03-27 14:17:47\n",
      "\n",
      "311 \t312 \t313 \t314 \t315 \t316 \t317 \t318 \t319 \t320 \t"
     ]
    }
   ],
   "source": [
    "sn=1\n",
    "#We use 'c' as both the serial number and the time count for bypassing anti-crawler\n",
    "datentime=str(date.today())\n",
    "datentime+=\" \"\n",
    "datentime+=datetime.now().strftime(\"%H:%M:%S\")\n",
    "print(\"Started at \"+datentime)\n",
    "with open('text_files/articles.txt','a',encoding=\"utf-8\")as outfile, open(\"text_files/article_links.txt\",\"r\",encoding='utf-8') as infile:\n",
    "#     outfile.write(\"This is a test\\n\")\n",
    "    for line in infile:\n",
    "        print(\"%d \\t\"%sn,end='')\n",
    "        if(sn%10==0):time.sleep(61)\n",
    "        page = requests.get(line,headers=headers)\n",
    "        soup = BeautifulSoup(page.content, 'lxml')\n",
    "        title=soup.find(class_=\"blog_entry-title\")\n",
    "        y=str(soup.find(class_=\"entry-content\"))\n",
    "        content=y.replace('\\n','').replace('\\r\\n','').replace('\\r','')\n",
    "        outfile.write(\"%d\\t%s\\n%s\\n\"%(sn,title,content))\n",
    "        sn+=1\n",
    "#         if(sn==10):break\n",
    "        if(sn%10==1):\n",
    "            datentime=str(date.today())\n",
    "            datentime+=\" \"\n",
    "            datentime+=datetime.now().strftime(\"%H:%M:%S\")\n",
    "            print(\"\\n\"+datentime+\"\\n\")\n",
    "datentime=str(date.today())\n",
    "datentime+=\" \"\n",
    "datentime+=datetime.now().strftime(\"%H:%M:%S\")\n",
    "print(\"\\nDone at \"+datentime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(\"https://www.lawyersnjurists.com/doclit/deed-agreement-dissolution-partnership/\",headers=headers)\n",
    "soup = BeautifulSoup(page.content, 'lxml')\n",
    "# print(BeautifulSoup(soup.body,\"html.parser\").prettify())\n",
    "x=soup.find(class_=\"blog_entry-title\")\n",
    "y=soup.find(class_=\"entry-content\")\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('csv_files/articles.csv', 'w') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"SN\",\"Title\", \"Content\"])\n",
    "    writer.writerow([1, x,y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=1\n",
    "s=\"Test\"\n",
    "with open('csv_files/list.csv','w')as out, open(\"text_files/test.txt\",\"r\") as f:\n",
    "    writer=csv.writer(out)\n",
    "    writer.writerow([\"SN\",\"Title\", \"Content\"])\n",
    "    for line in f:\n",
    "        writer.writerow([c,s,line.splitlines()])\n",
    "        c+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
