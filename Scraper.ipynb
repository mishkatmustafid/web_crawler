{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports and essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime,date\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn=1\n",
    "#We use 'c' as both the serial number and the time count for bypassing anti-crawler\n",
    "datentime=str(date.today())\n",
    "datentime+=\" \"\n",
    "datentime+=datetime.now().strftime(\"%H:%M:%S\")\n",
    "print(datentime)\n",
    "with open('csv_files/articles.csv','w',encoding=\"utf-8\")as outfile, open(\"text_files/article_links.txt\",\"r\",encoding='utf-8') as infile:\n",
    "    writer=csv.writer(outfile)\n",
    "    writer.writerow([\"SN\",\"Title\", \"Content\"])\n",
    "    for line in infile:\n",
    "        print(\"%d \\t\"%sn,end='')\n",
    "        if(sn%10==0):time.sleep(61)\n",
    "        page = requests.get(line,headers=headers)\n",
    "        soup = BeautifulSoup(page.content, 'lxml')\n",
    "        title=soup.find(class_=\"blog_entry-title\")\n",
    "        y=str(soup.find(class_=\"entry-content\"))\n",
    "        content=y.replace('\\n','').replace('\\r\\n','').replace('\\r','')\n",
    "        writer.writerow([sn,title,content])\n",
    "        sn+=1\n",
    "        if(sn==10):break\n",
    "            \n",
    "datentime=str(date.today())\n",
    "datentime+=\" \"\n",
    "datentime+=datetime.now().strftime(\"%H:%M:%S\")\n",
    "print(\"\\n\"+datentime)\n",
    "\n",
    "# There are newline characters in the articles that were fetched. This causes unorganized csv structure.\n",
    "# We have to find a new for writerow to work without considering newline characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempting to parse into txt file (Successful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at 2021-03-28 14:13:10\n",
      "808 \t809 \t810 \t\n",
      "2021-03-28 14:13:42\n",
      "\n",
      "811 \t812 \t813 \t814 \t815 \t816 \t817 \t818 \t819 \t820 \t\n",
      "2021-03-28 14:15:16\n",
      "\n",
      "821 \t822 \t823 \t824 \t825 \t826 \t827 \t828 \t829 \t830 \t\n",
      "2021-03-28 14:16:53\n",
      "\n",
      "831 \t832 \t833 \t834 \t835 \t836 \t837 \t838 \t839 \t840 \t\n",
      "2021-03-28 14:18:29\n",
      "\n",
      "841 \t842 \t843 \t844 \t845 \t846 \t847 \t848 \t849 \t850 \t\n",
      "2021-03-28 14:19:57\n",
      "\n",
      "851 \t852 \t853 \t854 \t855 \t856 \t857 \t858 \t859 \t860 \t\n",
      "2021-03-28 14:21:35\n",
      "\n",
      "861 \t862 \t863 \t864 \t865 \t866 \t867 \t868 \t869 \t870 \t\n",
      "2021-03-28 14:22:59\n",
      "\n",
      "871 \t872 \t873 \t874 \t875 \t876 \t877 \t878 \t879 \t880 \t\n",
      "2021-03-28 14:24:36\n",
      "\n",
      "881 \t882 \t883 \t884 \t885 \t886 \t887 \t888 \t889 \t890 \t\n",
      "2021-03-28 14:26:16\n",
      "\n",
      "891 \t892 \t893 \t894 \t895 \t896 \t897 \t898 \t899 \t900 \t\n",
      "2021-03-28 14:28:01\n",
      "\n",
      "901 \t902 \t903 \t904 \t905 \t906 \t907 \t908 \t909 \t910 \t\n",
      "2021-03-28 14:29:38\n",
      "\n",
      "911 \t912 \t913 \t914 \t915 \t916 \t917 \t918 \t919 \t920 \t\n",
      "2021-03-28 14:31:13\n",
      "\n",
      "921 \t922 \t923 \t924 \t925 \t926 \t927 \t928 \t929 \t930 \t\n",
      "2021-03-28 14:32:51\n",
      "\n",
      "931 \t932 \t933 \t934 \t935 \t936 \t937 \t938 \t939 \t940 \t\n",
      "2021-03-28 14:34:34\n",
      "\n",
      "941 \t942 \t943 \t944 \t945 \t946 \t947 \t948 \t949 \t950 \t\n",
      "2021-03-28 14:36:01\n",
      "\n",
      "951 \t952 \t953 \t954 \t955 \t956 \t957 \t958 \t959 \t960 \t\n",
      "2021-03-28 14:37:42\n",
      "\n",
      "961 \t962 \t963 \t964 \t965 \t966 \t967 \t968 \t969 \t970 \t\n",
      "2021-03-28 14:40:29\n",
      "\n",
      "971 \t972 \t973 \t974 \t975 \t976 \t977 \t978 \t979 \t980 \t\n",
      "2021-03-28 14:41:58\n",
      "\n",
      "981 \t982 \t983 \t984 \t985 \t986 \t987 \t988 \t989 \t990 \t\n",
      "2021-03-28 14:44:12\n",
      "\n",
      "991 \t992 \t993 \t994 \t995 \t996 \t997 \t998 \t999 \t1000 \t"
     ]
    }
   ],
   "source": [
    "sn=1\n",
    "#We use 'c' as both the serial number and the time count for bypassing anti-crawler\n",
    "datentime=str(date.today())\n",
    "datentime+=\" \"\n",
    "datentime+=datetime.now().strftime(\"%H:%M:%S\")\n",
    "print(\"Started at \"+datentime)\n",
    "with open('text_files/articles(9000-9999).txt','a',encoding=\"utf-8\")as outfile, open(\"text_files/article_links(9000-9999).txt\",\"r\",encoding='utf-8') as infile:\n",
    "#     outfile.write(\"This is a test\\n\")\n",
    "    for line in infile:\n",
    "        if(sn<808):\n",
    "            sn+=1\n",
    "            continue\n",
    "        print(\"%d \\t\"%sn,end='')\n",
    "#         if(sn%10==0):time.sleep(61)\n",
    "        page = requests.get(line,headers=headers)\n",
    "        time.sleep(6)\n",
    "        soup = BeautifulSoup(page.content, 'lxml')\n",
    "        title=soup.find(class_=\"blog_entry-title\")\n",
    "        y=str(soup.find(class_=\"entry-content\"))\n",
    "        content=y.replace('\\n','').replace('\\r\\n','').replace('\\r','')\n",
    "        outfile.write(\"%d\\t%s\\n%s\\n\"%(sn+9000,title,content))\n",
    "        sn+=1\n",
    "#         if(sn==10):break\n",
    "        if(sn%10==1):\n",
    "            datentime=str(date.today())\n",
    "            datentime+=\" \"\n",
    "            datentime+=datetime.now().strftime(\"%H:%M:%S\")\n",
    "            print(\"\\n\"+datentime+\"\\n\")\n",
    "datentime=str(date.today())\n",
    "datentime+=\" \"\n",
    "datentime+=datetime.now().strftime(\"%H:%M:%S\")\n",
    "print(\"\\nDone at \"+datentime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(\"https://www.lawyersnjurists.com/doclit/deed-agreement-dissolution-partnership/\",headers=headers)\n",
    "soup = BeautifulSoup(page.content, 'lxml')\n",
    "# print(BeautifulSoup(soup.body,\"html.parser\").prettify())\n",
    "x=soup.find(class_=\"blog_entry-title\")\n",
    "y=soup.find(class_=\"entry-content\")\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('csv_files/articles.csv', 'w') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"SN\",\"Title\", \"Content\"])\n",
    "    writer.writerow([1, x,y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=1\n",
    "s=\"Test\"\n",
    "with open('csv_files/list.csv','w')as out, open(\"text_files/test.txt\",\"r\") as f:\n",
    "    writer=csv.writer(out)\n",
    "    writer.writerow([\"SN\",\"Title\", \"Content\"])\n",
    "    for line in f:\n",
    "        writer.writerow([c,s,line.splitlines()])\n",
    "        c+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
