{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-testimony",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, random, re, time, os\n",
    "from bs4 import BeautifulSoup\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.190 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.isfile('test/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "possible-appointment",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "# with open(\"text_files/test.txt\",\"r\") as inputf:\n",
    "#     for line in inputf:\n",
    "#         urls.append(line)\n",
    "url=\"https://www.lawyersnjurists.com/doclitcat/legal-opinion/\"\n",
    "urls.append(url)\n",
    "for i in range(2,54):\n",
    "    link=url+'page/'+str(i)\n",
    "    urls.append(link)\n",
    "# for i in urls:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tr=1\n",
    "with open('text_files/litigation_links.txt','a',encoding='utf-8')as file:\n",
    "    while(urls!=[]):\n",
    "        url=urls.pop(0)\n",
    "        # s=url[-3:]\n",
    "        # if(s[1]=='/'):tr=s[-1:]\n",
    "        # elif(s[0]=='/'): tr=s[-2:]\n",
    "        page = requests.get(url,headers=headers)\n",
    "        if(page.status_code!=200):\n",
    "            urls.append(url)\n",
    "            print(\"%d %s %d\"%(page.status_code, tr, len(urls)))\n",
    "            continue\n",
    "        print(\"%d %s %d\"%(page.status_code, tr, len(urls)))\n",
    "        soup = BeautifulSoup(page.content, 'lxml')\n",
    "        y=str(soup)\n",
    "        i=y.find(\"href=\\\"\")+6\n",
    "        c=0\n",
    "        while(True):\n",
    "            c+=1\n",
    "            j=y[i:].find('\\\"')+i\n",
    "            if('doclit' in y[i:j]):file.write(\"%s\\n\"%y[i:j])\n",
    "            if(y.find(\"href=\\\"\",i)==-1):break\n",
    "            i=y.find(\"href=\\\"\",i)+6\n",
    "        file.write(\"Done %d\\n\"%c)\n",
    "        # content=y.replace('\\n','').replace('\\r\\n','').replace('\\r','')\n",
    "        # file.write(\"%s\\n\"%content)\n",
    "        # print(\"%s \\t\"%url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-phoenix",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-front",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "class timestamp:\n",
    "    def __init__(self):\n",
    "        \n",
    "    def now():\n",
    "    __str__:\n",
    "        return \"Hello\"\n",
    "print(now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-trauma",
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in urls:\n",
    "    page = requests.get(url,headers=headers)\n",
    "    soup = BeautifulSoup(page.content, 'lxml')\n",
    "    x=soup.find(class_=\"jtv-title\")\n",
    "    print(str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abroad-translation",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"http://bdlaws.minlaw.gov.bd/act-details-\"\n",
    "i=1\n",
    "ext=\".html\"\n",
    "for i in range(1,2000):\n",
    "    page = requests.get(url+str(i)+ext,headers=headers)\n",
    "    soup = BeautifulSoup(page.content, 'lxml')\n",
    "    if(\"404 Not Found\" in str(soup)):print(\"404\")\n",
    "    else: print(\"200\")\n",
    "    # x=soup.find(class_=\"service-list\")\n",
    "#     print(str(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-plymouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=str(x)\n",
    "s=\"\"\n",
    "i=y.find(\"</h2>\")\n",
    "print(ord(y[i+30]))\n",
    "for c in y[i+30:]:\n",
    "    if(c=='<'):break\n",
    "    if(ord(c)<58 and ord(c)>47):s+=c\n",
    "s=int(s)\n",
    "print(type(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-algebra",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://www.lawyersnjurists.com/sitemap.xml\"\n",
    "# page=requests.get(url,headers=headers)\n",
    "file=open('sitemaps/post-sitemap.htm','r')\n",
    "page=file.read()\n",
    "s=\"\"\n",
    "ls=[]\n",
    "flag=False\n",
    "for i in page:\n",
    "    if(i=='\\\"'):flag= not flag\n",
    "    if(flag):\n",
    "        s+=i\n",
    "    \n",
    "# soup=BeautifulSoup(file.content,'xml')\n",
    "# print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-height",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def func(x):\n",
    "    print(\"Starting \"+x+\"\\n\")\n",
    "    await asyncio.sleep(1)\n",
    "    print(\"Done \"+x)\n",
    "\n",
    "async def main():\n",
    "    x=input()\n",
    "    await func(x)\n",
    "    print(\"main\")\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "href=\"<a href=\\\"www.faceboowk.com/\\\"\"\n",
    "x=href.find(\"w\")\n",
    "while(x!=-1):\n",
    "    # y=href[x:].find(\"\\\"\")+x\n",
    "    print(x)\n",
    "    x=href.find(\"w\",x+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "with open('text_files/bank_pages.txt','r',encoding='utf-8') as file:\n",
    "    yfor line in file:          y=line         .find(\"href=\\\"\")+6\n",
    "                         (i!=-1):\n",
    "                                    .find('\\\"')+i\n",
    "                    t# e(\"%s\",y[i:j])\n",
    "        \n",
    "             print(y[i:j])            \"href=\\\"\",i)+6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "with open('text_files/legal_opinion_links.txt','r') as file:\n",
    "    for line in file:\n",
    "        s=\"\"\n",
    "        for i in line:\n",
    "            if(i!='\\n'):s+=i\n",
    "        print(\"\\\"\"+s+\"\\\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "sample=\"Hello world\"\n",
    "sample=sample[:5]\n",
    "print(sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python391jvsc74a57bd077eae0d149ba61c2d600661d89f90b4aa2630fec4556ce7a67e67a981106c5c1",
   "display_name": "Python 3.9.1 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "metadata": {
   "interpreter": {
    "hash": "77eae0d149ba61c2d600661d89f90b4aa2630fec4556ce7a67e67a981106c5c1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}